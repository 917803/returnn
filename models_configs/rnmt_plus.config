# Reference: https://arxiv.org/abs/1804.09849

EmbDim = 1024 # embedding dimension
LstmDim = 1024
EncValueTotalDim = 2048
AttNumHeads = 4
EncValuePerHeadDim = EncValueTotalDim // AttNumHeads


def add_enc_layer(d, inp, output, res_conn=False):
  """
    :param res_conn: True if residual connection is enabled and False otherwise (enabled for layers >= 3)
  """

  d[output + '_lstm_fw'] = {"class": "rec", "unit": "nativelstm2",
                            "n_out": LstmDim, "direction": 1, "from": [inp]}
  d[output + '_lstm_bw'] = {"class": "rec", "unit": "nativelstm2",
                            "n_out" : LstmDim, "direction": -1, "from": [inp]}

  # concatenate

  d[output + '_inp_concat'] = {"class": "copy", "from": [output + '_lstm_fw', output + '_lstm_bw']}

  d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_input_concat'], "dropout": 0.3}

  # add

  if res_conn:
    d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [output + '_inp_concat', output + '_ff_drop'],
                             "n_out": EncValueTotalDim}

    d[output + '_add'] = {"class": "copy", "from": [output + '_ff_out']}
  else:
    d[output + '_add'] = {"class": "copy", "from": [output + '_ff_drop']}

  # projection layer

  d[output + '_proj_out'] = {"class": "linear", "from": [output + '_add'], "activation": None, "with_bias": False,
                             "n_out": LstmDim}

  d[output] = {"class": "copy", "from": [output + '_proj_out']}


def add_dec_first_layer(d, db, inp, output):
    d[output + '_lstm_fw'] = {"class": "rec", "unit": "nativelstm2",
                              "n_out": LstmDim, "direction": 1, "from": [inp]}

    # Multi-head additive attention

    db['energy'] = {"class": "dot", "red1": -1, "red2": -1, "var1": "T", "var2": None,
                    "from": ["base:enc_ctx", "att_query"]},  # (B, H, enc-T, 1)

    db['att_weights'] = {"class": "softmax_over_spatial", "from": ["energy"],
                         "energy_factor": EncKeyPerHeadDim ** -0.5},  # (B, enc-T, H, 1)

    db['att0'] = {"class": "generic_attention", "weights": "att_weights", "base": "base:enc_value"},  # (B, H, V)

    db['att'] = {"class": "merge_dims", "axes": "except_batch", "from": ["att0"]},  # (B, H*V)

    db['s'] = {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": 1000},  # transform

    "readout_in": {"class": "linear", "from": ["prev:s", "prev:target_embed", "att"], "activation": None,
                   "n_out": 1000},  # merge + post_merge bias

    "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"]},

    "output_prob": {"class": "softmax", "from": ["readout"], "target": "bpe", "loss": "ce", "dropout": 0.3}

    d[output] = {"class": "copy", "from": [output + '_lstm_fw']}


def add_dec_layer(d, db, inp, output, res_conn=False):
    # concat

    d[output + '_inp_concat'] = {"class": "copy", "from": [inp, 'att_context']}

    # unidirectional lstm

    d[output + '_lstm_fw'] = {"class": "rec", "unit": "nativelstm2",
                              "n_out": LstmDim, "direction": 1, "from": [output + '_inp_drop']}

    # dropout

    d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_lstm_fw'], "dropout": 0.3}

    # add
    if res_conn:
      d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [output + '_inp_concat', output + '_ff_drop'],
                               "n_out": EncValueTotalDim}

      d[output + '_add'] = {"class": "copy", "from": [output + '_ff_out']}
    else:
      d[output + '_add'] = {"class": "copy", "from": [output + '_ff_drop']}


network = {
  "source_embed_raw": {"class": "linear", "activation": None, "with_bias": False, "n_out": EmbDim, "dropout": 0.3},
  "source_embed": {"class": "copy", "from": ["source_embed_raw"]},

  # add encoder layers

  "encoder": {"class": "layer_norm", "from": ["enc_N"]},

  "output": {"class": "rec", "from": [], "unit": {
    'output': {'class': 'choice', 'target': 'classes', 'beam_size': 12, 'from': ["output_prob"]},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    "target_embed_raw": {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'],
                     "n_out": EmbDim, "initial_output": 0},
    "target_embed": {"class": "dropout", "from": ["target_embed_raw"], "dropout": 0.3},

    # add decoder layers

    "decoder": {"class": "layer_norm", "from": ["dec_N"]},

    "output_prob": {
      "class": "softmax", "from": ["decoder", "att_context"], "dropout": 0.0,
      "target": "classes", "loss": "ce", "loss_opts": {"label_smoothing": 0.1},
      "with_bias": False
    }
  }, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3"},

  "decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        #"debug_print": True
    }
  }
}

# Add encoder layers

add_enc_layer(network, "source_embed", "enc_1")
add_enc_layer(network, "enc_1", "enc_2")
add_enc_layer(network, "enc_2", "enc_3", res_conn=True)
add_enc_layer(network, "enc_3", "enc_4", res_conn=True)
add_enc_layer(network, "enc_4", "enc_5", res_conn=True)
add_enc_layer(network, "enc_5", "enc_N", res_conn=True)

# Add decoder layers

net_out_unit = network["output"]["unit"]

add_dec_first_layer(network, net_out_unit, "target_embed", "dec_1")

add_dec_layer(network, net_out_unit, "dec_1", "dec_2")
add_dec_layer(network, net_out_unit, "dec_2", "dec_3", res_conn=True)
add_dec_layer(network, net_out_unit, "dec_3", "dec_4", res_conn=True)
add_dec_layer(network, net_out_unit, "dec_4", "dec_5", res_conn=True)
add_dec_layer(network, net_out_unit, "dec_5", "dec_6", res_conn=True)
add_dec_layer(network, net_out_unit, "dec_6", "dec_7", res_conn=True)
add_dec_layer(network, net_out_unit, "dec_7", "dec_N", res_conn=True)
