#!crnn/rnn.py
#Reference: https://arxiv.org/abs/1804.09849

import os
from subprocess import check_output
import numpy

my_dir = os.path.dirname(os.path.abspath(__file__))

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train'}"

num_outputs = {'data': [24236, 1], 'classes': [24214, 1]}
num_inputs = num_outputs["data"][0]

# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train', 'seq_ordering':'sorted'}" --get_num_seqs
num_seqs = {'train': 4218414, 'dev': 3003}

# Via JTP: 5 times seeing the whole train dataset is enough.
EpochSplit = 50
SeqOrderTrainBins = num_seqs["train"] // 1000
TrainSeqOrder = "laplace:%i" % SeqOrderTrainBins
if debug_mode:
    TrainSeqOrder = "default"

def get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    return {
        "class": "TranslationDataset",
        "path": "base/dataset",
        "file_postfix": data,
        "source_postfix": " </S>",
        "target_postfix": " </S>",
        "partition_epoch": epochSplit,
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default"),
        "estimated_num_seqs": (num_seqs.get(data, None) // epochSplit) if data in num_seqs else None}

train = get_dataset("train")
dev = get_dataset("dev")
dev_short = get_dataset("dev_short")
test_data = get_dataset("test")
eval_data = get_dataset("eval")
cache_size = "0"
window = 1

######################

class RNMTPlusNetwork:

    def __init__(self, enc_N=6, dec_N=8):
        self.enc_N = enc_N
        self.dec_N = dec_N

        self.EmbDim = 1024 # embedding dimension
        self.LstmDim = 1024
        self.EncProjDim = 1024

        self.EncKeyTotalDim = 1024
        self.AttNumHeads = 4
        self.EncKeyPerHeadDim = self.EncKeyTotalDim // self.AttNumHeads
        self.EncValueTotalDim = 1024
        self.EncValuePerHeadDim = self.EncValueTotalDim // self.AttNumHeads

        self.dropout = 0.3
        self.label_smoothing = 0.1

    def add_enc_layer(self, d, inp, output, res_conn):
        """
        :param res_conn: True if residual connection is enabled and False otherwise (enabled for layers >= 3)
        """

        d[output + '_inp_layernorm'] = {"class": "layer_norm", "from": [inp]}

        d[output + '_lstm_fw'] = {"class": "rec", "unit": "nativelstm2",
                                  "n_out": self.LstmDim, "direction": 1, "from": [output + '_inp_layernorm']}
        d[output + '_lstm_bw'] = {"class": "rec", "unit": "nativelstm2",
                                  "n_out": self.LstmDim, "direction": -1, "from": [output + '_inp_layernorm']}

        d[output + '_inp_concat'] = {"class": "copy", "from": [output + '_lstm_fw', output + '_lstm_bw']}

        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_inp_concat'], "dropout": self.dropout}

        if res_conn:
            d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_ff_drop']}

            d[output] = {"class": "copy", "from": [output + '_ff_out']}
        else:
            d[output] = {"class": "copy", "from": [output + '_ff_drop']}


    def add_dec_first_layer(self, d, output):

        d['prev_s_state'] = {"class": "get_last_hidden_state", "from": ["prev:" + output + "_s"], "n_out": self.LstmDim}
      
        d['prev_s_transformed'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], 
                                   "n_out": self.EncKeyTotalDim}
        
        d['energy_in'] = {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "prev_s_transformed"],
                          "n_out": self.EncKeyTotalDim} # (B, enc-T, D)

        d['energy_tanh'] = {"class": "activation", "activation": "tanh", "from": ["energy_in"]}

        d['energy'] = {"class": "linear", "activation": None, "with_bias": False, "from": ['energy_tanh'],
                       "n_out": self.AttNumHeads} # (B, enc-T, H, 1)

        d['att_weights'] = {"class": "softmax_over_spatial", "from": ["energy"]}  # (B, enc-T, H, 1)

        d['att0'] = {"class": "generic_attention", "weights": "att_weights", "base": "base:enc_value"}  # (B, H, V)

        d['att'] = {"class": "merge_dims", "axes": "except_batch", "from": ["att0"]}  # (B, H*V)

        #d['att_ctx'] = {"class": "linear", "activation": None, "with_bias": False, "n_out": self.EncValueTotalDim}

        d[output + '_s'] = {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], 
                            "n_out": self.LstmDim}
    
        d[output] = {"class": "linear", "from": ["prev:" + output + "_s", "prev:target_embed", "att"], "activation": None, 
                     "n_out": self.LstmDim}
        
    
    def add_dec_layer(self, d, inp, output, res_conn):
       
        d[output + '_inp_concat'] = {"class": "copy", "from": [inp, 'att']}
        
        d[output + '_prev_s_state'] = {"class": "get_last_hidden_state", "from": ["prev:" + output + "_s"], "n_out": self.LstmDim}
      
        d[output + '_s0'] = {"class": "linear", "from": [output + '_prev_s_state', output + '_inp_concat', 'att'], "activation": None, 
                             "n_out": self.LstmDim}    

        d[output + '_s'] = {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], 
                            "n_out": self.LstmDim}
    
        d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_s0'], "dropout": self.dropout}

        if res_conn:
            d[output + '_add'] = {"class": "combine", "kind": "add", "from": [inp, output + '_ff_drop']}

            d[output] = {"class": "copy", "from": [output + '_add']}
        else:
            d[output] = {"class": "copy", "from": [output + '_ff_drop']}

    
    def build_network(self):
        
        network = {
            "source_embed_raw": {"class": "linear", "activation": None, "with_bias": False, "n_out": self.EmbDim},

            "source_embed_drop": {"class": "dropout", "from": ["source_embed_raw"], "dropout": self.dropout},

            "source_embed": {"class": "copy", "from": ["source_embed_drop"]},

            # add encoder layers later through a separate method

            "encoder": {"class": "layer_norm", "from": ["enc_%d" % self.enc_N]},

            "encoder_proj": {"class": "linear", "activation": None, "with_bias": False, "n_out": self.EncValueTotalDim},

            "enc_ctx": {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder_proj"]},  # (B, enc-T, D)

            "enc_value": {"class": "split_dims", "axis": "F", "dims": (self.AttNumHeads, self.EncValuePerHeadDim),
                          "from": ["encoder_proj"]},  # (B, enc-T, H, D/H)

            "output": {"class": "rec", "from": [], "unit": {
                'output': {'class': 'choice', 'target': 'classes', 'beam_size': 12, 'from': ["output_prob"]},
                "end": {"class": "compare", "from": ["output"], "value": 0},
                "target_embed_raw": {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'],
                                     "n_out": self.EmbDim, "initial_output": 0},
                "target_embed": {"class": "dropout", "from": ["target_embed_raw"], "dropout": self.dropout},

                # add decoder layers later through a separate method

                "decoder": {"class": "layer_norm", "from": ["dec_%d" % self.dec_N]},

                "output_prob": {
                  "class": "softmax", "from": ["decoder", "att"], "dropout": 0.0,
                  "target": "classes", "loss": "ce", "loss_opts": {"label_smoothing": self.label_smoothing},
                  "with_bias": False
                }
            }, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3"},

            "decision": {
                "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
                "loss_opts": {
                    #"debug_print": True
                }
            }
        }

        # Add encoder layers
        self.add_enc_layer(network, "source_embed", "enc_1", False)
        for n in range(1, self.enc_N):
            self.add_enc_layer(network, "enc_%d" % n, "enc_%d" % (n+1), True if n >= 3 else False)
            
        # Add decoder layers
        net_out_unit = network["output"]["unit"]
        self.add_dec_first_layer(net_out_unit, "dec_1")
        for n in range(1, self.dec_N):
            self.add_dec_layer(net_out_unit, "dec_%d" % n, "dec_%d" % (n+1), True if n >= 3 else False)
        
        return network

######################

network = RNMTPlusNetwork().build_network()
search_output_layer = 'decision'

debug_print_layer_output_template = True
debug_add_check_numerics_on_output = True
debug_print_layer_output_shape = True

# trainer
tf_log_memory_usage = True
log_batch_size = True
batching = "random"
batch_size = 4096
max_seqs = 200
max_seq_length = 256
truncation = -1
num_epochs = 300
model = "net-model/network"
cleanup_old_models = True

gradient_clip = 5.0 # this should be adaptive
optimizer = {"class": "adam", "beta1":0.9, "beta2":0.999, "epsilon":1e-6}
gradient_noise = 0.0

learning_rate = 0.0001
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 10

newbob_multi_num_epochs = 10
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "newbob.data"

# log
log = "log/crnn.%s.log" % task
log_verbosity = 5
