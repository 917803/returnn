#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:
# base: /u/schamper/experiments/apptek/2018-06-04_es-en/config/esen/configs/trafo_config_512_ff2048_l6.py

import os
from subprocess import check_output
import numpy

my_dir = os.path.dirname(os.path.abspath(__file__))

debug_mode = False
if int(os.environ.get("DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train'}"
num_outputs = {'data': [24236, 1], 'classes': [24214, 1]}
num_inputs = num_outputs["data"][0]

# see: returnn/tools/dump-dataset.py "{'class':'TranslationDataset', 'path':'dataset', 'file_postfix':'train', 'seq_ordering':'sorted'}" --get_num_seqs
num_seqs = {'train': 4218414, 'dev': 3003}
# Via JTP: 5 times seeing the whole train dataset is enough.
EpochSplit = 50
SeqOrderTrainBins = num_seqs["train"] // 1000
TrainSeqOrder = "laplace:%i" % SeqOrderTrainBins
if debug_mode:
    TrainSeqOrder = "default"

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if int(os.environ.get("CF_NOT_FOR_LOCAL", "1")) and check_output(["hostname"]).strip() in ["cluster-cn-211", "sulfid"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip()
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


def get_dataset(data):
    epochSplit = {"train": EpochSplit}.get(data, 1)
    return {
        "class": "TranslationDataset",
        "path": "base/dataset",
        "file_postfix": data,
        "source_postfix": " </S>",
        "target_postfix": " </S>",
        "partition_epoch": epochSplit,
        "seq_ordering": {"train": TrainSeqOrder, "dev": "sorted"}.get(data, "default"),
        "estimated_num_seqs": (num_seqs.get(data, None) // epochSplit) if data in num_seqs else None}

train = get_dataset("train")
dev = get_dataset("dev")
dev_short = get_dataset("dev_short")
test_data = get_dataset("test")
eval_data = get_dataset("eval")
cache_size = "0"
window = 1


FFDim = 2048
EncKeyTotalDim = 512
AttNumHeads = 8
EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads
EncValueTotalDim = 512
EncValuePerHeadDim = EncValueTotalDim // AttNumHeads


# consider: /work/smt3/bahar/debug/returnn/t2t-test/02.07-2018-test/returnn/config-new.py

def add_trafo_enc_layer(d, inp, output):
  d[output + '_self_att_laynorm'] = {"class": "layer_norm", "from": [inp]}
  d[output + '_self_att_att'] = {"class": "self_attention", "num_heads": AttNumHeads, "total_key_dim": EncKeyTotalDim,
                             "n_out": EncValueTotalDim, "from": [output + '_self_att_laynorm'], "attention_left_only": False}
  d[output + '_self_att_lin'] = {"class": "linear", "activation": None, "with_bias": False,
                                   "from": [output + '_self_att_att'], "n_out": EncValueTotalDim}
  d[output + '_self_att_drop'] = {"class": "dropout", "from": [output + '_self_att_lin'], "dropout": 0.1}
  d[output + '_self_att_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_self_att_drop'],
                                 "n_out": EncValueTotalDim}
  #####
  d[output + '_ff_laynorm'] = {"class": "layer_norm", "from": [output + '_self_att_out']}
  d[output + '_ff_conv1'] = {"class": "linear", "activation": "relu", "with_bias": True, "from": [output + '_ff_laynorm'],
                            "n_out": FFDim}
  d[output + '_ff_conv2'] = {"class": "linear", "activation": None, "with_bias": True, "from": [output + '_ff_conv1'],
                            "n_out": EncValueTotalDim}
  d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_ff_conv2'], "dropout": 0.1}
  d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [output + '_self_att_out', output + '_ff_drop'],
                           "n_out": EncValueTotalDim}
  d[output] = {"class": "copy", "from": [output + '_ff_out']}


def add_trafo_dec_layer(db, d, inp, output):
  d[output + '_self_att_laynorm'] = {"class": "layer_norm", "from": [inp]}
  d[output + '_self_att_att'] = {"class": "self_attention", "num_heads": AttNumHeads, "total_key_dim": EncKeyTotalDim,
                               "n_out": EncValueTotalDim, "from": [output + '_self_att_laynorm'], "attention_left_only": True}
  d[output + '_self_att_lin'] = {"class": "linear", "activation": None, "with_bias": False,
                                 "from": [output + '_self_att_att'], "n_out": EncValueTotalDim}
  d[output + '_self_att_drop'] = {"class": "dropout", "from": [output + '_self_att_lin'], "dropout": 0.1}
  d[output + '_self_att_out'] = {"class": "combine", "kind": "add", "from": [inp, output + '_self_att_drop'],
                                 "n_out": EncValueTotalDim}
  #####
  d[output + '_att_laynorm'] = {"class": "layer_norm", "from": [output + '_self_att_out']}
  d[output + '_att_query0'] = {"class": "linear", "activation": None, "with_bias": False, "from": [output + '_att_laynorm'],
                               "n_out": EncValueTotalDim}
  d[output + '_att_query'] = {"class": "split_dims", "axis": "F", "dims": (AttNumHeads, EncKeyPerHeadDim),
                              "from": [output + '_att_query0']}  # (B, H, D/H)
  db[output + '_att_key0'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                              "n_out": EncKeyTotalDim}  # (B, enc-T, D)
  db[output + '_att_value0'] = {"class": "linear", "activation": None, "with_bias": False, "from": ["encoder"],
                                "n_out": EncValueTotalDim}
  db[output + '_att_key'] = {"class": "split_dims", "axis": "F", "dims": (AttNumHeads, EncKeyPerHeadDim),
                             "from": [output + '_att_key0']}  # (B, enc-T, H, D/H)
  db[output + '_att_value'] = {"class": "split_dims", "axis": "F", "dims": (AttNumHeads, EncValuePerHeadDim),
                               "from": [output + '_att_value0']}  # (B, enc-T, H, D'/H)
  d[output + '_att_energy'] = {"class": "dot", "red1": -1, "red2": -1, "var1": "T", "var2": "T?",
                           "from": ['base:' + output + '_att_key', output + '_att_query']}  # (B, H, enc-T, 1)
  d[output + '_att_weights'] = {"class": "softmax_over_spatial", "from": [output + '_att_energy'],
                                "energy_factor": EncKeyPerHeadDim ** -0.5}  # (B, enc-T, H, 1)
  d[output + '_att0'] = {"class": "generic_attention", "weights": output + '_att_weights',
                         "base": 'base:' + output + '_att_value'}  # (B, H, V)
  d[output + '_att_merge'] = {"class": "merge_dims", "axes": "static", "from": [output + '_att0']}  # (B, H*V) except_batch
  d[output + '_att_lin'] = {"class": "linear", "activation": None, "with_bias": False, "from": [output + '_att_merge'],
                          "n_out": EncValueTotalDim}
  d[output + '_att_drop'] = {"class": "dropout", "from": [output + '_att_lin'], "dropout": 0.1}
  d[output + '_att_out'] = {"class": "combine", "kind": "add", "from": [output + '_self_att_out', output + '_att_drop'],
                         "n_out": EncValueTotalDim}
  #####
  d[output + '_ff_laynorm'] = {"class": "layer_norm", "from": [output + '_att_out']}
  d[output + '_ff_conv1'] = {"class": "linear", "activation": "relu", "with_bias": True, "from": [output + '_ff_laynorm'],
                            "n_out": FFDim}
  d[output + '_ff_conv2'] = {"class": "linear", "activation": None, "with_bias": True, "from": [output + '_ff_conv1'],
                            "n_out": EncValueTotalDim}
  d[output + '_ff_drop'] = {"class": "dropout", "from": [output + '_ff_conv2'], "dropout": 0.1}
  d[output + '_ff_out'] = {"class": "combine", "kind": "add", "from": [output + '_att_out', output + '_ff_drop'],
                           "n_out": EncValueTotalDim}
  d[output] = {"class": "copy", "from": [output + '_ff_out']}



# network
# (also defined by num_inputs & num_outputs)
network = {
  "source_embed_raw": {"class": "linear", "activation": None, "with_bias": False, "n_out": EncValueTotalDim},
  "source_embed_weighted": {"class": "eval", "from": ["source_embed_raw"], "eval": "source(0) * (%i**0.5)" % EncValueTotalDim},
  "source_embed_with_pos": {"class": "positional_encoding", "add_to_input": True, "from": ["source_embed_weighted"],  "dropout": 0.1},
  "source_embed": {"class": "copy", "from": ["source_embed_with_pos"]},

  ## trafo layer added later

  "encoder": {"class": "layer_norm", "from": ["enc_N"]},

  "output": {"class": "rec", "from": [], "unit": {
    'output': {'class': 'choice', 'target': 'classes', 'beam_size': 12, 'from': ["output_prob"]},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'target_embed_raw': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'],
                     "n_out": EncValueTotalDim, "initial_output": 0},  # there seems to be no <s> in t2t, they seem to use just the zero vector
    "target_embed_weighted": {"class": "eval", "from": ["prev:target_embed_raw"], "eval": "source(0) * (%i**0.5)" % EncValueTotalDim},
    "target_embed_with_pos": {"class": "positional_encoding", "add_to_input": True, "from": ["target_embed_weighted"]},
    "target_embed": {"class": "dropout", "from": ["target_embed_with_pos"], "dropout": 0.1},

    ## trafo layer added later

    "decoder": {"class": "layer_norm", "from": ["dec_N"]},

    "output_prob": {
      "class": "softmax", "from": ["decoder"], "dropout": 0.0,
      "target": "classes", "loss": "ce", "loss_opts": {"label_smoothing": 0.1},
      "with_bias": False
    }

  }, "target": "classes", "max_seq_len": "max_len_from('base:encoder') * 3"},

  "decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes",
    "loss_opts": {
        #"debug_print": True
    }
  }

}

add_trafo_enc_layer(network, "source_embed", "enc_1")
add_trafo_enc_layer(network, "enc_1", "enc_2")
add_trafo_enc_layer(network, "enc_2", "enc_3")
add_trafo_enc_layer(network, "enc_3", "enc_4")
add_trafo_enc_layer(network, "enc_4", "enc_5")
add_trafo_enc_layer(network, "enc_5", "enc_N")
add_trafo_dec_layer(network, network["output"]["unit"], "target_embed", "dec_1")
add_trafo_dec_layer(network, network["output"]["unit"], "dec_1", "dec_2")
add_trafo_dec_layer(network, network["output"]["unit"], "dec_2", "dec_3")
add_trafo_dec_layer(network, network["output"]["unit"], "dec_3", "dec_4")
add_trafo_dec_layer(network, network["output"]["unit"], "dec_4", "dec_5")
add_trafo_dec_layer(network, network["output"]["unit"], "dec_5", "dec_N")


search_output_layer = "decision"
debug_print_layer_output_template = True

# https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py

# Check /u/zeineldeen/changes.txt

# trainer
tf_log_memory_usage = True
log_batch_size = True
batching = "random"
batch_size = 2048
max_seqs = 200
max_seq_length = 256  #75
#chunking = ""  # no chunking
truncation = -1
num_epochs = 300
model = "net-model/network"
cleanup_old_models = True

def custom_construction_algo(idx, net_dict):
    # For debugging, use: python3 ./crnn/Pretrain.py config...
    orig_num_lstm_layers = 0
    while "lstm%i_fw" % orig_num_lstm_layers in net_dict:
        orig_num_lstm_layers += 1
    num_lstm_layers = idx + 1  # idx starts at 0. start with 1 layer
    # Use label smoothing only at the very end.
    net_dict["output"]["unit"]["output_prob"]["loss_opts"]["label_smoothing"] = 0
    if num_lstm_layers == orig_num_lstm_layers:
        return net_dict
    if num_lstm_layers >= orig_num_lstm_layers:
        return None
    # Leave the last layer as-is, but only modify its source.
    net_dict["encoder"]["from"] = ["lstm%i_fw" % (num_lstm_layers - 1), "lstm%i_bw" % (num_lstm_layers - 1)]
    # Delete non-used lstm layers. This is not explicitly necessary but maybe nicer.
    for i in range(num_lstm_layers, orig_num_lstm_layers):
        del net_dict["lstm%i_fw" % i]
        del net_dict["lstm%i_bw" % i]
    return net_dict

#pretrain = {"repetitions": 5, "construction_algo": custom_construction_algo}

gradient_clip = 0
#gradient_clip_global_norm = 1.0

# adam = True
# optimizer_epsilon = 1e-9

optimizer = {"class":Adam, "beta1":0.9, "beta2":0.997, "epsilon":1e-9}

#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
gradient_noise = 0.0  # maybe try...
# accum_grad_multiple_step maybe try...
# "constant*linear_warmup*rsqrt_decay*rsqrt_hidden_size"

learning_rate = 0.2

learning_rates = list(numpy.exp(numpy.linspace(numpy.log(0.0003), numpy.log(learning_rate), num=15)))  # warmup
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 10
newbob_multi_num_epochs = 10
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "newbob.data"

# log
#log = "| /u/zeyer/dotfiles/system-tools/bin/mt-cat.py >> log/crnn.seq-train.%s.log" % task
log = "log/crnn.%s.log" % task
log_verbosity = 5
